---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.7
  kernelspec:
    display_name: R
    name: ir
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE, cache=FALSE}
    knitr::opts_chunk$set(
      echo = TRUE,        # Display code in the output
      warning = FALSE,    # Suppress warnings
      message = FALSE,    # Suppress messages
      results = "hide"    #Exclude  results unless specified
    )

```

# Machine Learning Final Project
*Authors: David Costa, Lucas Gaspar*


## Project Description

The goal of this project is to apply multiple machine/statistical learning techniques we've learned over the course of the semester in MATH 4050 to a fictional dataset of 8,000 observations with 31 features. We intend to use unsupervised learning (Hierarchical clustering) to conduct some preliminary examination and help guide the decision of which features to use as response variables (1 categorical and 1 numerical). We will then use best practices to develop Linear/Logistic regression, Decision Tree, Random Forest, and Support Vector Machine models. Along the way we will use various tuning methods, validation techiniques, and dimensional reduction techniques to ensure that our models are well fit, and robust.

The data used in the project is a fictional dataset containing 31 features about a given household. It contains 16 categorical and 15 numerical variables:

| Feature     | Description                                                                 |
|-------------|-----------------------------------------------------------------------------|
| **urbrur**  | Whether the household is located in urban or rural location                 |
| **hhsize**  | Household size (number of members)                                          |
| **statocc** | Whether the household rents, owns, or has free occupancy                    |
| **rooms**   | Number of rooms in the household                                             |
| **bedrooms**| Number of bedrooms in the household                                          |
| **floor**   | Floor number where the household is located (if applicable)                 |
| **walls**   | Type of walls in the household                                               |
| **roof**    | Type of roof in the household                                                |
| **electricity** | Whether the household has electricity                                    |
| **cook_fuel** | Type of cooking fuel used in the household                                |
| **phone**   | Whether the household has a phone                                            |
| **cell**    | Whether the household has a mobile phone                                     |
| **car**     | Whether the household has a car                                              |
| **bicycle** | Whether the household has a bicycle                                          |
| **motorcycle** | Whether the household has a motorcycle                                    |
| **refrigerator** | Whether the household has a refrigerator                                |
| **tv**      | Whether the household has a television                                       |
| **radio**   | Whether the household has a radio                                            |
| **bank**    | Whether the household has a bank account                                     |
| **exp_01**  | Annual spending on Food and non-alcoholic beverages                          |
| **exp_02**  | Annual spending on Alcoholic beverages, tobacco and narcotics                |
| **exp_03**  | Annual spending on Clothing and footwear                                     |
| **exp_04**  | Annual spending on Housing, water, electricity, gas and other fuels          |
| **exp_05**  | Annual spending on Furnishing, household equipment and routine maintenance   |
| **exp_06**  | Annual spending on Health                                                    |
| **exp_07**  | Annual spending on Transport                                                 |
| **exp_08**  | Annual spending on Communication                                             |
| **exp_09**  | Annual spending on Recreation and culture                                    |
| **exp_10**  | Annual spending on Education                                                 |
| **exp_11**  | Annual spending on Catering and accommodation services                       |
| **exp_12**  | Annual spending on Miscellaneous goods and services                         |


```{r}
df <- read.csv("https://raw.githubusercontent.com/costad3atwit/Machine_Learning_Final/refs/heads/main/projectData.csv")
```

```{r}
#install.packages("gower")
library(gower)
#install.packages("StatMatch")
library(StatMatch)
```

## Examining and Making the Dataset Usable
```{r}
#Rename confusing variables

df <- df %>% rename(
  spend_food = exp_01,
  spend_alcohol = exp_02,
  spend_clothes = exp_03,
  spend_housing = exp_04,
  spend_furnishing = exp_05,
  spend_health = exp_06,
  spend_transport = exp_07,
  spend_communication = exp_08,
  spend_recreation = exp_09,
  spend_education = exp_10,
  spend_catering = exp_11,
  spend_misc = exp_12
)

df$X <- NULL
```


```{r}
df$tv <- as.factor(df$tv)
df$urbrur <- as.factor(df$urbrur)
df$statocc <- as.factor(df$statocc)
df$floor <- as.factor(df$floor)
df$walls <- as.factor(df$walls)
df$roof <- as.factor(df$roof)
df$electricity <- as.factor(df$electricity)
df$cook_fuel <- as.factor(df$cook_fuel)
df$phone <- as.factor(df$phone)
df$cell <- as.factor(df$cell)
df$car <- as.factor(df$car)
df$bicycle <- as.factor(df$bicycle)
df$motorcycle <- as.factor(df$motorcycle)
df$refrigerator <- as.factor(df$refrigerator)
df$radio <- as.factor(df$radio)
df$bank <- as.factor(df$bank)

str(df)
```

```{r, results = "markup"}
head(df)
```
### FAMD Testing

```{r, results = "markup"}
# Load required packages
library(FactoMineR)
library(factoextra)

# Ensure row names are unique before any processing
#rownames(df) <- paste0("row_", 1:nrow(df))

df_famd <- df

# Perform the FAMD analysis with error handling
res.famd <- FAMD(df_famd, graph = FALSE, ncp = 5)

# Scree plot should work regardless of row names
print("Generating scree plot...")
scree_plot <- fviz_screeplot(res.famd)
print(scree_plot)

# Try visualization with row names explicitly set
print("Generating individual plot...")

# Extract the individual coordinates
ind_coords <- as.data.frame(res.famd$ind$coord)

# Make sure the row names are properly set
rownames(ind_coords) <- paste0("row_", 1:nrow(ind_coords))

# Manual plotting as an alternative to fviz_famd_ind
library(ggplot2)

# Get the cos2 values for coloring
cos2 <- rowSums(res.famd$ind$cos2[,1:2])

# Create a data frame for plotting
plot_df <- data.frame(
Dim1 = ind_coords[,1],
  Dim2 = ind_coords[,2],
  cos2 = cos2
)

# Create the plot
ind_plot <- ggplot(plot_df, aes(x = Dim1, y = Dim2, color = cos2)) +
  geom_point(size = 3) +
  scale_color_gradient2(low = "#00AFBB", mid = "#E7B800", high = "#FC4E07",
                       midpoint = median(cos2)) +
  theme_minimal() +
  labs(title = "Individuals - FAMD",
       x = paste0("Dim 1 (", round(res.famd$eig[1,2], 1), "%)"),
       y = paste0("Dim 2 (", round(res.famd$eig[2,2], 1), "%)"))

print(ind_plot)

# Try the variable plot
print("Generating variable plot...")
var_plot <- fviz_famd_var(res.famd, repel = TRUE)
print(var_plot)

  
# Display the structure of your data for debugging
print("Structure of the first few rows of your data:")
print(str(head(df_famd)))
```
Scree Plot
- represents how much variance explained in how many dimensions. ~30% in 2 is not very good
- not sure how to apply this yet

Individuals
- hotter means better represented in those dimensions

Variables
- kinda shows how variables are grouped i think
- it looks like theres some patterns of what the variables are and how well one thing explains them



## Unsupervised Data Analysis: Clustering

We use cluster analysis to help pick an interesting response variable via hierarchical clustering

- Tune “cut-off height” for clusters (dissimilarity)
- To determine what we would like to predict (1 categorical and 1 numerical) and understand the data a little better.
- Use Gower distance w/ hierarchical cus mixed data (chapter 12 lab)
- Include formula for gower distance in rmd
- Explain how hierarchical clustering converges to 1 or more clusters


```{r}
gower_df = gower.dist(df)
```

```{r}
library(cluster)
```

We'll use the Partitions Around Mediods (PAM) Method of clustering to look for best clustering and subsequently the most impactful features. We'll use those features to decide what to predict.

```{r}
pam_result_2 <- pam(gower_df, k=2, diss=TRUE)
sil_2 = silhouette(pam_result_2)
pam_result_3 <- pam(gower_df, k=3, diss=TRUE)
sil_3 = silhouette(pam_result_3)
pam_result_4 <- pam(gower_df, k=4, diss=TRUE)
sil_4 = silhouette(pam_result_4)
pam_result_5 <- pam(gower_df, k=5, diss=TRUE)
sil_5 = silhouette(pam_result_5)
pam_result_6 <- pam(gower_df, k=6, diss=TRUE)
sil_6 = silhouette(pam_result_6)
pam_result_7 <- pam(gower_df, k=7, diss=TRUE)
sil_7 = silhouette(pam_result_7)
```

```{r, results = "markup"}
par(mfrow = c(1,2))
plot(sil_2, main = "Silhouette Chart with 2 clusters",
     xlab = "", sub = "", cex = 0.9, 
     col = c("red", "blue"),  # Add colors for different clusters
     border = NA,             # Remove borders around individual lines
     do.n.k = TRUE,           # Show number of observations and clusters
     do.clus.stat = TRUE)     # Show cluster statistics
plot(sil_3, main = "Silhouette Chart with 3 clusters",
     xlab = "", sub = "", cex = 0.9, 
     col = c("red", "blue","green"),  # Add colors for different clusters
     border = NA,             # Remove borders around individual lines
     do.n.k = TRUE,           # Show number of observations and clusters
     do.clus.stat = TRUE)     # Show cluster statistics
```
```{r, results = "markup"}
par(mfrow = c(1,2))
plot(sil_4, main = "Silhouette Chart with 4 clusters",
     xlab = "", sub = "", cex = 0.9, 
     col = c("red", "blue","green", "purple"),  # Add colors for different clusters
     border = NA,             # Remove borders around individual lines
     do.n.k = TRUE,           # Show number of observations and clusters
     do.clus.stat = TRUE)     # Show cluster statistics
plot(sil_5, main = "Silhouette Chart with 5 clusters",
     xlab = "", sub = "", cex = 0.9, 
     col = c("red", "blue","green", "purple", "cyan"),  # Add colors for different clusters
     border = NA,             # Remove borders around individual lines
     do.n.k = TRUE,           # Show number of observations and clusters
     do.clus.stat = TRUE)     # Show cluster statistics
```

```{r, results = "markup"}
par(mfrow = c(1,2))
plot(sil_6, main = "Silhouette Chart with 6 clusters",
     xlab = "", sub = "", cex = 0.9, 
     col = c("red", "blue","green", "purple", "cyan", "black"),  # Add colors for different clusters
     border = NA,             # Remove borders around individual lines
     do.n.k = TRUE,           # Show number of observations and clusters
     do.clus.stat = TRUE)     # Show cluster statistics
plot(sil_7, main = "Silhouette Chart with 7 clusters",
     xlab = "", sub = "", cex = 0.9, 
     col = c("red", "blue","green", "purple", "cyan", "black", "brown"),  # Add colors for different clusters
     border = NA,             # Remove borders around individual lines
     do.n.k = TRUE,           # Show number of observations and clusters
     do.clus.stat = TRUE)     # Show cluster statistics
```


It looks like our best clustering occured with only 2 clusters because the average silhouette width was the highest among each cluster there. We'll continue using the 2 cluster model.

```{r, results = "markup"}
df$cluster <- pam_result_2$clustering

# Create empty lists to store results
numeric_results <- list()
categorical_results <- list()

# Loop through each variable (except the cluster variable)
for(var_name in names(df)[1:31]) {

  if(is.numeric(df[[var_name]])) {
    # For numeric variables
    # Calculate mean, median, standard deviation by cluster
    stats_by_cluster <- aggregate(df[[var_name]] ~ cluster, data=df,
                                 FUN=function(x) c(mean=mean(x, na.rm=TRUE),
                                                  median=median(x, na.rm=TRUE),
                                                  sd=sd(x, na.rm=TRUE)))

    # Calculate standardized difference (Cohen's d)
    means <- c(stats_by_cluster[1,2][1], stats_by_cluster[2,2][1])
    sds <- c(stats_by_cluster[1,2][3], stats_by_cluster[2,2][3])
    pooled_sd <- sqrt((sds[1]^2 + sds[2]^2)/2)
    effect_size <- abs(means[1] - means[2])/pooled_sd

    # Perform t-test between clusters
    t_test_result <- t.test(df[[var_name]] ~ df$cluster)

    # Store results
    numeric_results[[var_name]] <- data.frame(
      variable = var_name,
      cluster1_mean = means[1],
      cluster2_mean = means[2],
      mean_difference = abs(means[1] - means[2]),
      effect_size = effect_size,
      p_value = t_test_result$p.value
    )

  } else {
    # For categorical variables
    # Create contingency table
    cont_table <- table(df$cluster, df[[var_name]])

    # Calculate proportions within each cluster
    prop_table <- prop.table(cont_table, margin=1)

    # Chi-square test
    chi_test <- chisq.test(cont_table)

    # Calculate Cramer's V (effect size for categorical variables)
    n <- sum(cont_table)
    cramers_v <- sqrt(chi_test$statistic / (n * (min(dim(cont_table)) - 1)))

    # Store results
    categorical_results[[var_name]] <- data.frame(
      variable = var_name,
      chi_square = chi_test$statistic,
      p_value = chi_test$p.value,
      cramers_v = cramers_v
    )
  }
}

# Combine results
numeric_df <- do.call(rbind, numeric_results)
categorical_df <- do.call(rbind, categorical_results)

# Sort by effect size/statistical significance
numeric_df <- numeric_df[order(-numeric_df$effect_size),]
categorical_df <- categorical_df[order(-categorical_df$cramers_v),]

# Print top 10 most differentiating variables
print("Top 10 Numeric Variables:")
print(head(numeric_df, 10))
print("Top 10 Categorical Variables:")
print(head(categorical_df, 10))
```

Based on the top ten most meaningful features derived above through clustering, we've decided to move forward with our supervised learning techniques predicting on **spend_housing** *(spending on housing AND utilities)*, **and tv** *(television)* because their effect size is high and the findings may be interesting

```{r}
#Before we move on we need to remove the cluster variable from the dataframe so we don't skew our other models
df$cluster = NULL

```

## Subsetting the Data for Regression

We'll need to set aside some test data before we work with any supervised learning techniques so that we can perform accurate validation

```{r}
sample = sample(nrow(df), nrow(df) * .75)
df.train <- df[sample, ]
df.test <- df[-sample, ]
```

## Linear Regression

In this section, we'll develop a linear regression model to predict housing expenditures (`spend_housing`) based on various household characteristics. Linear regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data.

The general form of the linear regression equation is:

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$$

Where:
- $Y$ is the response variable (spend_housing)
- $\beta_0$ is the intercept
- $\beta_1, \beta_2, ..., \beta_p$ are the coefficients
- $X_1, X_2, ..., X_p$ are the predictor variables
- $\epsilon$ is the error term

The training process involves minimizing the Mean Squared Error (MSE), which is calculated as:

$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

Where $y_i$ is the actual value and $\hat{y}_i$ is the predicted value.

### Feature Selection using Forward Stepwise Selection

First, we'll apply forward stepwise selection to identify the most important predictors for our model:

```{r, results = "markup"}
# Load necessary libraries for linear regression
library(leaps)
library(caret)

# Set seed for reproducibility
set.seed(123)

# Create a formula excluding the response variable
predictors <- setdiff(names(df), c("spend_housing", "cluster"))
formula_all <- as.formula(paste("spend_housing ~", paste(predictors, collapse = " + ")))

# Null model (for comparison)
null_model <- lm(spend_housing ~ 1, data = df.train)
null_mse_train <- mean(residuals(null_model)^2)
cat("Null model training MSE:", null_mse_train, "\n")

# Forward stepwise selection
forward_selection <- suppressWarnings(step(lm(spend_housing ~ 1, data = df.train), 
                         direction = "forward", 
                         scope = formula_all, 
                         trace = FALSE))

# Display the selected variables
cat("Variables selected by forward selection:\n")
print(formula(forward_selection))
```

### Fitting the Linear Model with Selected Features

Now, we'll fit the linear regression model using the features selected by forward stepwise selection:

```{r, results = "markup"}
# Fit the linear model with selected predictors
model_forward <- lm(formula(forward_selection), data = df.train)
summary_forward <- summary(model_forward)

# Display model summary and key statistics
print(summary_forward)

# Calculate training MSE
mse_train <- mean(residuals(model_forward)^2)
cat("Training MSE:", mse_train, "\n")
cat("R-squared:", summary_forward$r.squared, "\n")
cat("Adjusted R-squared:", summary_forward$adj.r.squared, "\n")
```

### Cross-Validation for Model Evaluation

We'll use K-fold cross-validation to evaluate our model and compare it with the null model:

```{r, results = "markup"}
# Function to calculate MSE
mse <- function(actual, predicted) {
  mean((actual - predicted)^2)
}

# Set up 10-fold cross-validation
k <- 10
set.seed(456)
folds <- createFolds(df.train$spend_housing, k = k, list = TRUE, returnTrain = FALSE)

# Function to evaluate models using k-fold CV
cv_evaluation <- function(model_formula) {
  cv_results <- data.frame(fold = integer(), train_mse = numeric(), test_mse = numeric())
  
  for (i in 1:k) {
    fold_test <- df.train[folds[[i]], ]
    fold_train <- df.train[-folds[[i]], ]
    
    model <- lm(model_formula, data = fold_train)
    
    # Predictions
    train_pred <- predict(model, fold_train)
    test_pred <- predict(model, fold_test)
    
    # Calculate MSE
    train_mse <- mse(fold_train$spend_housing, train_pred)
    test_mse <- mse(fold_test$spend_housing, test_pred)
    
    cv_results <- rbind(cv_results, data.frame(fold = i, train_mse = train_mse, test_mse = test_mse))
  }
  
  return(cv_results)
}

# Evaluate models:
# 1. Null model (just intercept)
null_model_formula <- as.formula("spend_housing ~ 1")
null_cv_results <- cv_evaluation(null_model_formula)

# 2. Forward selection model
forward_cv_results <- cv_evaluation(formula(forward_selection))

# Calculate average MSE across folds
null_avg_mse <- mean(null_cv_results$test_mse)
forward_avg_mse <- mean(forward_cv_results$test_mse)

cat("Null Model - Average CV Test MSE:", null_avg_mse, "\n")
cat("Forward Selection Model - Average CV Test MSE:", forward_avg_mse, "\n")
cat("Improvement (% reduction in MSE):", (null_avg_mse - forward_avg_mse) / null_avg_mse * 100, "%\n")
```

### Final Model Evaluation on Test Set

Now, we'll evaluate our model on the held-out test set to assess its predictive performance:

```{r, results = "markup"}
# Make predictions on the test set
predictions <- predict(model_forward, df.test)

# Calculate test MSE
test_mse <- mse(df.test$spend_housing, predictions)
cat("Test MSE:", test_mse, "\n")

# Calculate null model test MSE for comparison
null_pred <- rep(mean(df.train$spend_housing), nrow(df.test))
null_test_mse <- mse(df.test$spend_housing, null_pred)
cat("Null Model Test MSE:", null_test_mse, "\n")

# Calculate R-squared on test data
tss <- sum((df.test$spend_housing - mean(df.test$spend_housing))^2)
rss <- sum((df.test$spend_housing - predictions)^2)
test_r_squared <- 1 - (rss/tss)
cat("Test R-squared:", test_r_squared, "\n")

# Visualize actual vs predicted values
plot(df.test$spend_housing, predictions, 
     main = "Actual vs. Predicted Housing Expenditure",
     xlab = "Actual", ylab = "Predicted",
     pch = 16, col = "blue", cex = 0.8)
abline(0, 1, col = "red", lwd = 2)
```

### Interpretation of Results

The multiple regression model reveals several significant predictors of housing expenditure. 

```{r, results = "markup"}
# Display top 5 most significant predictors (t-value)
coef_table <- as.data.frame(summary_forward$coefficients)
coef_table$variable <- rownames(coef_table)
coef_table <- coef_table[order(-abs(coef_table$`t value`)), ]
cat("Top 5 most significant predictors of housing expenditure:\n")
print(head(coef_table[, c("variable", "Estimate", "t value", "Pr(>|t|)")], 5))

```

The multiple regression reveals insights into what drives housing expenditure:

1. **Model Performance**: The model explains approximately 93.1 of the variance in housing expenditure, which is substantially better than the null model which in this case was near 0%.

2. **Key Predictors**: The most significant predictors include spend_communication, spend_misc, rooms, wallsBrick, and spend_catering. For example, each additional dollar spent on communication is associated with an increase of $1.302 in housing expenditure, holding all other variables constant.

3. **Validation**: Cross-validation confirms the model is robust, with consistent performance across different subsets of the data.

4. **Prediction Accuracy**: The test MSE of 395058.4 indicates our model can predict housing expenditure with reasonable accuracy (Null Model Test MSE: 5440908), with a 92.105% improvement over the null model.
This improvement was calculated as $\text{Percentage Improvement}=( \frac{5004201−395058.4}{5004201})*100 ≈ 92.105%$


## Logistic Regression
In this section, we'll develop a logistic regression model to predict whether a household has a television (`tv` = "Yes" or "No") based on various household characteristics. Logistic regression is used when the response variable is categorical, making it appropriate for binary classification problems like this one.

The logistic regression model uses the logistic (sigmoid) function to estimate the probability of the positive ("Yes") class given the on inputs X:

$$P(Y=1|X) = \frac{e^{\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p}}$$

Where:
- $P(Y=1|X)$ is the probability that the response equals 1 (having a TV)
- $\beta_0$ is the intercept
- $\beta_1, \beta_2, ..., \beta_p$ are the coefficients
- $X_1, X_2, ..., X_p$ are the predictor variables

### Step 1: Feature Selection using Forward Stepwise Selection

First, we'll apply forward stepwise selection to identify the most important predictors:


```{r, results = "markup"}

# Create a formula excluding the response variable
predictors <- setdiff(names(df), c("tv"))
formula_all <- as.formula(paste("tv ~", paste(predictors, collapse = " + ")))

# Null model (for comparison)
null_model_log <- glm(tv ~ 1, data = df.train, family = "binomial")
summary(null_model_log)

# Forward stepwise selection for logistic regression

forward_selection_log <- step(glm(tv ~ 1, data = df.train, family = "binomial"), 
                             direction = "forward", 
                             scope = formula_all, 
                             trace = FALSE)

# Display the selected variables
cat("Variables selected by forward selection for tv prediction:\n")
print(formula(forward_selection_log))
```


### Step 2: Fitting the Logistic Regression Model with Selected Features

Now, we'll fit the logistic regression model using the features selected by forward stepwise selection:

```{r, results = "markup"}
# Fit the logistic model with selected predictors
model_forward_log <- glm(formula(forward_selection_log), data = df.train, family = "binomial")
summary_forward_log <- summary(model_forward_log)

# Display model summary
print(summary_forward_log)

# Make predictions on training data
prob_train <- predict(model_forward_log, df.train, type = "response")
pred_train <- ifelse(prob_train > 0.5, "Yes", "No")
pred_train <- factor(pred_train, levels = levels(df.train$tv))

# Calculate training accuracy
conf_matrix_train <- confusionMatrix(pred_train, df.train$tv)
print(conf_matrix_train)

# Calculate AIC and deviance
cat("AIC:", summary_forward_log$aic, "\n")
cat("Null Deviance:", summary_forward_log$null.deviance, 
    "on", summary_forward_log$df.null, "degrees of freedom\n")
cat("Residual Deviance:", summary_forward_log$deviance, 
    "on", summary_forward_log$df.residual, "degrees of freedom\n")
```

### Step 3: Cross-Validation for Model Evaluation

We'll use K-fold cross-validation to evaluate our model and compare it with the null model:

```{r, results = "markup"}
# Set up 10-fold cross-validation
k <- 10
set.seed(101)
folds <- createFolds(df.train$tv, k = k, list = TRUE, returnTrain = FALSE)

# Function to evaluate logistic models using k-fold CV
cv_evaluation_log <- function(model_formula) {
  cv_results <- data.frame(fold = integer(), accuracy = numeric(), 
                           precision = numeric(), recall = numeric(),
                           f1_score = numeric(), auc = numeric())
  
  for (i in 1:k) {
    fold_test <- df.train[folds[[i]], ]
    fold_train <- df.train[-folds[[i]], ]
    
    model <- glm(model_formula, data = fold_train, family = "binomial")
    
    # Predictions
    probs <- predict(model, fold_test, type = "response")
    preds <- ifelse(probs > 0.5, "Yes", "No")
    preds <- factor(preds, levels = levels(fold_test$tv))
    
    # Calculate metrics
    cm <- confusionMatrix(preds, fold_test$tv)
    accuracy <- cm$overall["Accuracy"]
    
    # Precision, recall, F1 for "Yes" class (assuming "Yes" is the positive class)
    tp <- cm$table[2, 2]  # True positives (predicted Yes, actual Yes)
    fp <- cm$table[2, 1]  # False positives (predicted Yes, actual No)
    fn <- cm$table[1, 2]  # False negatives (predicted No, actual Yes)
    
    precision <- tp / (tp + fp)
    recall <- tp / (tp + fn)
    f1_score <- 2 * precision * recall / (precision + recall)
    
    # For AUC, we need the ROC curve
    library(pROC)
    roc_obj <- roc(fold_test$tv, probs)
    auc_value <- auc(roc_obj)
    
    cv_results <- rbind(cv_results, data.frame(
      fold = i, accuracy = accuracy, precision = precision,
      recall = recall, f1_score = f1_score, auc = auc_value
    ))
  }
  
  return(cv_results)
}

# Evaluate models:
# 1. Null model (just intercept)
null_model_formula_log <- as.formula("tv ~ 1")
null_cv_results_log <- cv_evaluation_log(null_model_formula_log)

# 2. Forward selection model
forward_cv_results_log <- cv_evaluation_log(formula(forward_selection_log))

# Calculate average metrics across folds
null_avg_metrics <- colMeans(null_cv_results_log[, -1])
forward_avg_metrics <- colMeans(forward_cv_results_log[, -1])

# Display results
cat("Null Model - Average CV Metrics:\n")
print(null_avg_metrics)
cat("\nForward Selection Model - Average CV Metrics:\n")
print(forward_avg_metrics)
cat("\nImprovement (accuracy):", 
    (forward_avg_metrics["accuracy"] - null_avg_metrics["accuracy"]) * 100, 
    "percentage points\n")
```

### Step 4: Final Model Evaluation on Test Set

Now, we'll evaluate our model on the held-out test set to assess its predictive performance:

```{r, results = "markup"}
# Make predictions on the test set
prob_test <- predict(model_forward_log, df.test, type = "response")
pred_test <- ifelse(prob_test > 0.5, "Yes", "No")
pred_test <- factor(pred_test, levels = levels(df.test$tv))

# Calculate test metrics
conf_matrix_test <- confusionMatrix(pred_test, df.test$tv)
print(conf_matrix_test)

# Create ROC curve
library(pROC)
roc_test <- roc(df.test$tv, prob_test)
auc_test <- auc(roc_test)

# Plot ROC curve
plot(roc_test, main = paste("ROC Curve (AUC =", round(auc_test, 3), ")"),
     col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

# Null model performance for comparison
null_pred_test <- rep(levels(df.test$tv)[which.max(table(df.train$tv))], nrow(df.test))
null_pred_test <- factor(null_pred_test, levels = levels(df.test$tv))
null_acc_test <- mean(null_pred_test == df.test$tv)
cat("Null Model Test Accuracy:", null_acc_test, "\n")
cat("Forward Model Test Accuracy:", conf_matrix_test$overall["Accuracy"], "\n")
```

### Step 5: Interpretation of Results

The logistic regression model reveals several significant predictors of television ownership:

```{r, results = "markup"}
# Identify the most influential predictors (by absolute z-value)
coef_summary <- as.data.frame(summary_forward_log$coefficients)
coef_summary$Variable <- rownames(coef_summary)
coef_summary <- coef_summary[order(-abs(coef_summary$`z value`)), ]
cat("\nTop 5 most significant predictors (by z-value):\n")
print(head(coef_summary[, c("Variable", "Estimate", "z value", "Pr(>|z|)")], 5))
```

Our logistic regression analysis reveals important insights into what factors are associated with television ownership:

1. **Model Performance**: The model achieves an accuracy of approximately 89.7% on the test set, which is substantially better than the null model's accuracy of 69.25%.

2. **Key Predictors**: The most significant predictors include spend_recreation, bankYes, refigeratorYes, spend_transport, and electricityYes. For example:
   - Having a refrigerator increases the chance of having a TV by 0.912%
   - Every unit increase in recreation spending increases the chance of having a TV by 0.013% *within the maximum*

3. **Validation**: Cross-validation confirms the model is robust, with consistent performance across different subsets of the data.

4. **Discriminative Power**: The AUC (Area under ROC curve) of 0.948 indicates that the model has excellent discriminative ability in distinguishing between households with and without TVs. This in combination with the accuracy of 89.7% indicates that the model is strong.

## Decision Tree
- Only use for the categorical prediction
- Explain DT creation process briefly (Recursive Binary Splitting) (create partitions in feature space to minimize overall average partition-wise Node purity)
- Fit tree (overgrown)
- Cost complexity pruning
 - Make sure to try out different costs
- Include explanation and equation for Gini Index (Node purity)
- Plot DT

## Decision Tree
We will use two different Decision tree techniques when looking at our data. Additionally, we will only be using these trees to predict categorical values despite the fact that they can be used for both types of data. We will start with making a traditional Decision tree that we will prune back, and then use the Random Forest technique. There will all predict whether or not an entry has a TV.

### Step 1: Create and test Tree

We will create a single tree that makes its decisions on each split based on creating the most pure regions.

```{r, results = "markup"}
library(tree)
set.seed(112)
df.tree <- tree(df.train$tv ~ ., data = df.train)
summary(df.tree)

plot(df.tree)
text(df.tree, pretty = 0)
title(main = "TV Purchase Decision Tree - Unpruned")
```
### Step 2: Test tree error

Next, we can test this tree to see what its error rate along with what it classified correctly.

```{r, results = "markup"}
df.tree.pred <- predict(df.tree, df.test, type ="class")
table(Predicted = df.tree.pred, Actual = df.test$tv)
cat("Test Error Rate: ", mean(df.tree.pred != df.test$tv))
```


### Step 3: Calculate Cross Validation for Pruning 

This shows us that the tree experiences the least error relative to its size when the tree size = 4. Even if there is a slight decrease in error as we include more decisions in the tree, the benefit will not be worth the computation cost. This is known as **Cost-Complexity-Pruning**

```{r, results = "markup"}
cv.df.tree <- suppressWarnings(cv.tree(df.tree, FUN = prune.misclass))
plot(cv.df.tree$size, cv.df.tree$dev, type = "b", main = "Cross-Validataion",
     xlab = "Tree Size", ylab = "Error")
```

### Step 4: Prune and Finalize Tree

Now, we can prune the tree to the size that was show to be optimal by our Cross-Validation. This is our final result.

```{r, results = "markup"}
df.tree.pruned <- prune.misclass(df.tree, best = 4)
summary(df.tree.pruned)

df.tree.pruned.pred <- predict(df.tree.pruned, df.test, type ="class")
table(Predicted = df.tree.pruned.pred, Actual = df.test$tv)
cat("Test Error Rate: ", mean(df.tree.pruned.pred != df.test$tv))

plot(df.tree.pruned)
text(df.tree.pruned, pretty = 0)
title(main = "TV Purchase Decision Tree - Final")
```
This decision tree is superior to our initial one due to its shorter length. This is now an extremely simple and robust model that can be used to predict whether or not a TV is owned.

## Random Forest
We will now create a large assortment of trees to predict the same variable using the Random Forest method. 

Step 1: Create Model

```{r, results = "markup"}
library(randomForest)

df.rf <- randomForest(df.train$tv ~., data = df.train, importance = TRUE)
df.rf
```
### Step 2: Calculate Test Error

What is shown above is the error on the training data. Now we want to find the testing error.

```{r, results = "markup"}
df.rf.pred <- predict(df.rf, newdata = df.test)
table(Predicted = df.rf.pred, Actual = df.test$tv)
cat("Test Error rate:", mean(df.rf.pred != df.test$tv))
```
Using this method, we can see a much lower error rate of 9.55% compared the 13.6% present in the single tree model. While the error is lower, there are downsides to this technique such as the lack of a tree for visualization and the significantly longer computation time.


## Support Vector Machine

In this section, we'll implement a Support Vector Machine (SVM) model to predict television ownership (`tv` = "Yes" or "No"). SVMs find the optimal hyperplane that creates the maximum margin between classes in the feature space.

SVM is ultimately an optimization problem to maximize the margin between points of different classes. This is achieved by adjusting factors such as the cost parameter $C$ which controls the trade-off between margin width and misclassification, and $\gamma$ which determines how far the influence of a single training example reaches, affecting the curvature of the decision boundary (only used in non-linear kernels).

### (Re)Loading Required Libraries

```{r}
library(e1071)
library(caret)
```

### Initial Model with Default Parameters

First, we'll fit an SVM model with default parameters:

```{r, results = "markup"}

# Set seed for reproducibility
set.seed(123)

#We use a much smaller subset of data so that CV tuning will run in reasonable time. The support vectors control the entire model so this doesn't introduce bias as much as in other models
#sample = sample(nrow(df), nrow(df) * .10)
#df.train.svm <- df[sample,]
#df.test.svm <- df[-sample,]

# Commented the section above for final knitting so all the models are trained on the same data
# Know that the small subset was used while working on project
df.train.svm = df.train
df.test.svm = df.test

# Fit SVM with default (untuned) parameters
svm_default <- svm(tv ~ ., data = df.train.svm, 
                  kernel = "radial", 
                  probability = TRUE)

print(summary(svm_default))

# Make predictions on the test set
svm_default_pred <- predict(svm_default, df.test.svm)

# Evaluate model performance
conf_matrix_default <- confusionMatrix(svm_default_pred, df.test.svm$tv)
print(conf_matrix_default)

# Calculate error rate
error_rate_default <- 1 - conf_matrix_default$overall['Accuracy']
cat("Error rate with default parameters:", error_rate_default, "\n")
```


### Tuning SVM Parameters to Avoid Overfitting/Underfitting

To ensure our model isn't overfit or underfit, we'll tune two key parameters:

1. **Cost parameter (C)**: Controls the trade-off between having a smooth decision boundary and classifying training points correctly. 
   - Higher values of C: More complex model, risk of overfitting
   - Lower values of C: Simpler model, risk of underfitting

2. **Gamma parameter**: Controls the influence radius of training examples (for radial kernel).
   - Higher gamma: More complex model, risk of overfitting
   - Lower gamma: Simpler model, risk of underfitting

We'll use cross-validation based tuning to find optimal values:

```{r, results = "markup"}
# Perform 10-fold (default for tune()) cross-validation for cost and gamma tuning
set.seed(456)
svm_tune <- tune(
  svm,
  tv ~.,
  data = df.train.svm,
  kernel = "radial",
  ranges = list(cost = c(5,10,50,100,1000), gamma = c(0.001,0.1)),
  
)

# Display best parameters
print(svm_tune$best.parameters)
cat("Best performance during tuning (error rate):", svm_tune$best.performance, "\n")

# Plot tuning results
plot(svm_tune)
```

### Training Final SVM Model with Optimal Parameters

Now we'll train our final model using the optimal parameters identified through tuning and evaluate it's performance on held out data to compare with other models:

```{r, results = "markup"}
# Extract best parameters
best_cost <- svm_tune$best.parameters$cost
best_gamma <- svm_tune$best.parameters$gamma

# Train final model with best parameters
svm_final <- svm(
  tv ~ .,
  data = df.train.svm,
  kernel = "radial",
  cost = best_cost,
  gamma = best_gamma,
  probability = TRUE
)

# Make predictions on the test set
svm_pred <- predict(svm_final, df.test.svm)

# Evaluate final model performance
conf_matrix_final <- confusionMatrix(svm_pred, df.test.svm$tv)
print(conf_matrix_final)

# Calculate and compare error rates
cat("Cost value:", best_cost , "\n")
cat("Gamma value:", best_gamma , "\n")
error_rate_final <- 1 - conf_matrix_final$overall['Accuracy']
cat("Error rate with tuned parameters:", error_rate_final, "\n")
cat("Error rate with default parameters:", error_rate_default, "\n")
cat("Improvement:", (error_rate_default - error_rate_final) * 100, "percentage points\n")
```

### Learning Curve Analysis to Confirm Model Fit

To ensure our model is neither overfit nor underfit, we'll examine how performance changes with training data size:

```{r, results = "markup"}
# Creating a learning curve
train_sizes <- seq(0.1, 1, by = 0.1)
train_errors <- numeric(length(train_sizes))
test_errors <- numeric(length(train_sizes))

set.seed(101)
for (i in 1:length(train_sizes)) {
  # Sample data
  n_train <- floor(train_sizes[i] * nrow(df.train.svm))
  train_indices <- sample(1:nrow(df.train.svm), n_train)
  train_subset <- df.train.svm[train_indices, ]
  
  # Train model on subset
  svm_subset <- svm(
    tv ~ .,
    data = train_subset,
    kernel = "radial",
    cost = best_cost,
    gamma = best_gamma
  )
  
  # Evaluate on training and test sets
  train_pred <- predict(svm_subset, train_subset)
  test_pred <- predict(svm_subset, df.test.svm)
  
  # Calculate error rates
  train_errors[i] <- 1 - mean(train_pred == train_subset$tv)
  test_errors[i] <- 1 - mean(test_pred == df.test.svm$tv)
}

# Plot learning curve
plot(train_sizes, train_errors, type = "b", col = "blue", 
     ylim = c(0, max(c(train_errors, test_errors)) * 1.1),
     xlab = "Training Data Proportion", ylab = "Error Rate",
     main = "Learning Curve for SVM Model")
lines(train_sizes, test_errors, type = "b", col = "red")
legend("topright", legend = c("Training Error", "Test Error"), 
       col = c("blue", "red"), lty = 1, pch = 1)
```

### ROC Curve and AUC

Let's evaluate the model's discriminative ability using ROC and AUC:

```{r, results = "markup"}
# Get probability predictions
svm_probs <- predict(svm_final, df.test.svm, probability = TRUE)
svm_probs <- attr(svm_probs, "probabilities")[, "Yes"]

# Generate ROC curve
library(pROC)
roc_svm <- roc(df.test.svm$tv, svm_probs)
auc_svm <- auc(roc_svm)

# Plot ROC curve
plot(roc_svm, main = paste("ROC Curve for SVM (AUC =", round(auc_svm, 3), ")"),
     col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")
```

### Comparison with Other Models

Finally, let's compare the SVM's performance with our previous models:

```{r, results = "markup"}
# Create comparison table for all models (accuracy or error rate)
model_comparison <- data.frame(
  Model = c("Logistic Regression", "Decision Tree", "RandomForest", "SVM"),
  Accuracy = c(
    conf_matrix_test$overall["Accuracy"],  # From Logistic Regression section
    mean(df.tree.pred == df.test$tv),     # From Decision Tree section
    mean(predict(df.rf, df.test) == df.test$tv),  # From Random Forest section
    conf_matrix_final$overall["Accuracy"]  # SVM
  )
)

# Sort by accuracy
model_comparison <- model_comparison[order(-model_comparison$Accuracy), ]
print(model_comparison)

# Visualize model comparison
barplot(model_comparison$Accuracy, names.arg = model_comparison$Model, 
        main = "Model Accuracy Comparison", col = "steelblue",
        ylim = c(0.8, 1), las = 2)
abline(h = seq(0, 1, by = 0.1), col = "gray", lty = 2)
```

### Interpretation of SVM Results

The Support Vector Machine model for predicting television ownership has shown strong performance:

1. **Model Tuning**: We tuned the cost (C) and gamma parameters through grid search and 5-fold cross-validation. The optimal parameters were found to be cost = 5 and gamma = 0.1, striking a balance between model complexity and generalization ability.

2. **Performance**: The tuned SVM achieved an accuracy of 90.65% on the test set, which is the best out of all the other models (logistic regression, decision tree, and random forest), barely beating out random forest. However given the computationally expensive nature of fitting an SVM model it may be worth considering use of the RF model over the SVM in this situation (only 0.2% accuracy increase from RF).

3. **Avoiding Overfitting/Underfitting**: 
   - The learning curve analysis shows that as we increase training data size, the training error increases and test error decreases, indicating the hyperparameters are well tuned because the model doesn't become overfit (training error decreasing and test error increasing) when we include larger portions of the data.
   - The gap between training and test error is roughly 2% (and shrinks with more data), suggesting the model is neither excessively overfit nor underfit.
   - Parameter tuning via cross-validation further ensured optimal regularization.

4. **Discriminative Power**: The AUC of 0.951 demonstrates the model's excellent ability to distinguish between households with and without TVs.

Overall, the SVM model provides a robust prediction of television ownership, balancing complexity with generalization through careful parameter tuning. Its performance compared to the other tested models in this project makes it one of the better models for predicting TV ownership.